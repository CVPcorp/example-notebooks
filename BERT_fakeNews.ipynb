{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W3vj3dcZc7mg"
   },
   "source": [
    "# **Using a BERT Model to Predict Fake News**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 131001,
     "status": "ok",
     "timestamp": 1575306094954,
     "user": {
      "displayName": "Nick Newman",
      "photoUrl": "",
      "userId": "12535314105832740514"
     },
     "user_tz": 300
    },
    "id": "xd54-OCFdLcW",
    "outputId": "66f1dd76-84fa-43f1-d40e-a32c0a5d127f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-0402aa15-8c2b-49dc-b331-3ae22b111d91\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-0402aa15-8c2b-49dc-b331-3ae22b111d91\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving news.csv to news.csv\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "41J6PcG2R1fO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.utils.data as data_utils\n",
    "import torch.optim as optim\n",
    "import gc #garbage collector for gpu memory \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "miCTWgjqTmOu"
   },
   "source": [
    "#### The BERT package (transformers) has to be installed and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dkdmK8PvdtV5"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N1xJfKjrpdDa"
   },
   "source": [
    "#### Import the library specific to running BERT models on PyTorch. The transformers package using the existing PyTorch infrastructure to recreate the BERT model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vAnUF7Q1TK9t"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IxyHPxvpqID"
   },
   "source": [
    "#### Read in the news data through the csv file. The following columns are not relevant for this endeavor:\n",
    "\n",
    "*   ID - this is meaningless and could cause overfitting\n",
    "*   Title - for this experiment we'll choose to omit it\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7UtncyxgTL0C"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "news_data = pd.read_csv(\"news.csv\",header=1)\n",
    "\n",
    "news_data.columns = ['id','title','text','target_names','target']\n",
    "del news_data['id']\n",
    "del news_data['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xHvsxeI9Lrm6"
   },
   "source": [
    "#### This is a preview of the data once the irrelevant columns have been removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135296,
     "status": "ok",
     "timestamp": 1575306105958,
     "user": {
      "displayName": "Nick Newman",
      "photoUrl": "",
      "userId": "12535314105832740514"
     },
     "user_tz": 300
    },
    "id": "BIjmluGoLoAd",
    "outputId": "54a0e3d1-0bfe-4d91-bffa-8c986e4da136"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target_names</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nI’m not an immigrant, but my grandparents ...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Share This Baylee Luciani (left), Screenshot o...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A Czech stockbroker who saved more than 650 Je...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hillary Clinton and Donald Trump made some ina...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Iranian negotiators reportedly have made a las...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CEDAR RAPIDS, Iowa — “I had one of the most wo...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text target_names  target\n",
       "0  Google Pinterest Digg Linkedin Reddit Stumbleu...         FAKE       0\n",
       "1  U.S. Secretary of State John F. Kerry said Mon...         REAL       1\n",
       "2  — Kaydee King (@KaydeeKing) November 9, 2016 T...         FAKE       0\n",
       "3  It's primary day in New York and front-runners...         REAL       1\n",
       "4    \\nI’m not an immigrant, but my grandparents ...         FAKE       0\n",
       "5  Share This Baylee Luciani (left), Screenshot o...         FAKE       0\n",
       "6  A Czech stockbroker who saved more than 650 Je...         REAL       1\n",
       "7  Hillary Clinton and Donald Trump made some ina...         REAL       1\n",
       "8  Iranian negotiators reportedly have made a las...         REAL       1\n",
       "9  CEDAR RAPIDS, Iowa — “I had one of the most wo...         REAL       1"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mD0JNbcnqHar"
   },
   "source": [
    "#### The transformers package comes with a tokenizer for each model. We'll use the BERT tokenizer here and a BERT base model where the text isn't modified for case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135029,
     "status": "ok",
     "timestamp": 1575306106146,
     "user": {
      "displayName": "Nick Newman",
      "photoUrl": "",
      "userId": "12535314105832740514"
     },
     "user_tz": 300
    },
    "id": "f7nUdh2bTUTv",
    "outputId": "ea6a1aaa-2ed2-42c6-bd71-e974351ef90e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 213450/213450 [00:00<00:00, 5579422.84B/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6UUt_BlpTwKv"
   },
   "source": [
    "#### Tokenizing the data so that each sentence is split into words and symbols. Also '[CLS]' and '[SEP]' to the beginning and end of every article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fTU3g0utTWih"
   },
   "outputs": [],
   "source": [
    "tokenized_df = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], news_data['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JUeF9eIgT_71"
   },
   "source": [
    "#### The max input length for a BERT algorithm is 512, so we'll have to pad each article to this length or cut it short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bUH2aaTuT9mE"
   },
   "outputs": [],
   "source": [
    "totalpadlength = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qC2N768vUOd9"
   },
   "source": [
    "#### We need to get the index for each token so that we can map them to be put in a matrix embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u_J8Nx-3UIMN"
   },
   "outputs": [],
   "source": [
    "indexed_tokens = list(map(tokenizer.convert_tokens_to_ids, tokenized_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dBO7O7InULoP"
   },
   "outputs": [],
   "source": [
    "index_padded = np.array([xi+[0]*(totalpadlength-len(xi)) for xi in indexed_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HUfJOQbQU7Wp"
   },
   "source": [
    "#### Setting up an array with the binary target variable values\n",
    "* 0 = FAKE\n",
    "* 1 = REAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6fQrmWLdUeJf"
   },
   "outputs": [],
   "source": [
    "target_variable = news_data['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8GNfTz1DVBID"
   },
   "source": [
    "#### Creating dictionaries that map the tokens to the index and the index to the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GShqmScmU5Ma"
   },
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for l in tokenized_df:\n",
    "  all_words.extend(l)\n",
    "all_indices = []\n",
    "for i in indexed_tokens:\n",
    "  all_indices.extend(i)\n",
    "\n",
    "word_to_ix = dict(zip(all_words, all_indices))\n",
    "ix_to_word = dict(zip(all_indices, all_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "biX1BjKtVGb_"
   },
   "source": [
    "#### The BERT algorithm relies on masking to help it learn and to prevent overfitting, so we'll add this to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mWzKFbd2VEGh"
   },
   "outputs": [],
   "source": [
    "mask_variable = [[float(i>0) for i in ii] for ii in index_padded]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a8NxoJyMVPnz"
   },
   "source": [
    "#### This loads the data into train and test dataloaders, which for PyTorch is necessary to iterate through the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0EGCS8atVN5w"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 14\n",
    "def format_tensors(text_data, mask, labels, batch_size):\n",
    "    X = torch.from_numpy(text_data)\n",
    "    X = X.long()\n",
    "    mask = torch.tensor(mask)\n",
    "    y = torch.from_numpy(labels)\n",
    "    y = y.long()\n",
    "    tensordata = data_utils.TensorDataset(X, mask, y)\n",
    "    loader = data_utils.DataLoader(tensordata, batch_size=batch_size, shuffle=False)\n",
    "    return loader\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(index_padded, target_variable, \n",
    "                                                    test_size=0.1, random_state=42)\n",
    "\n",
    "train_masks, test_masks, _, _ = train_test_split(mask_variable, index_padded, \n",
    "                                                       test_size=0.1, random_state=42)\n",
    "\n",
    "trainloader = format_tensors(X_train, train_masks, y_train,BATCH_SIZE)\n",
    "testloader = format_tensors(X_test, test_masks, y_test, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-g05NLG2VmMe"
   },
   "source": [
    "#### This is a sample batch from the trainloader. The first tensor contains the embeddings for the articles, the second tensor contains the masking information, and the third tensor contains the target variables for each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 210696,
     "status": "ok",
     "timestamp": 1575306185760,
     "user": {
      "displayName": "Nick Newman",
      "photoUrl": "",
      "userId": "12535314105832740514"
     },
     "user_tz": 300
    },
    "id": "zfSbQJciVaJY",
    "outputId": "99deea91-120d-4bb6-a32d-c4cb426a8a60"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  101,  5096, 13053,  ...,  2400,   119,   102],\n",
       "         [  101,  1118,  5728,  ...,  1142,  3507,   102],\n",
       "         [  101, 11255,   170,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  1109,  2383,  ...,  1103,  5637,   102],\n",
       "         [  101, 18653, 11922,  ...,  1343,  1107,   102],\n",
       "         [  101,   107,   146,  ...,     0,     0,     0]]),\n",
       " tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.]]),\n",
       " tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1])]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJhkInO_V03B"
   },
   "source": [
    "\n",
    "### Now it's time to create the BERT Model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P0zItZ3SV8Uy"
   },
   "source": [
    "#### The BERT model architecture is shown below. This is a BERT base-cased model, which means it has 12 BERT transformer layers, 768 hidden layers, 12 heads, 110M parameters, and is pre-trained on cased English text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 221739,
     "status": "ok",
     "timestamp": 1575306197918,
     "user": {
      "displayName": "Nick Newman",
      "photoUrl": "",
      "userId": "12535314105832740514"
     },
     "user_tz": 300
    },
    "id": "zMzcRtjmVghX",
    "outputId": "c6f4f44b-b1fc-480f-b44f-12cdd384dbd5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:00<00:00, 216101.59B/s]\n",
      "100%|██████████| 435779157/435779157 [00:07<00:00, 57478490.93B/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-cased')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RTfAMHLqWCbm"
   },
   "source": [
    "#### Creating a function to compute the accuracy after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o8I-otafV5rn"
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(model, dataloader, device):\n",
    "    tqdm()\n",
    "    model.eval()\n",
    "    correct_preds, num_samples = 0,0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(dataloader)):\n",
    "            token_ids, masks, labels = tuple(t.to(device) for t in batch)\n",
    "            _, yhat = model(input_ids=token_ids, attention_mask=masks, labels=labels)\n",
    "            prediction = (torch.sigmoid(yhat[:,1]) > 0.5).long()\n",
    "            num_samples += labels.size(0)\n",
    "            correct_preds += (prediction==labels.long()).sum()\n",
    "            del token_ids, masks, labels #memory\n",
    "        torch.cuda.empty_cache() #memory\n",
    "        gc.collect() # memory\n",
    "        return correct_preds.float()/num_samples*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z5SbPkIiWSup"
   },
   "source": [
    "#### Now we iterate through the dataset, updating the model weights at each instance. Since BERT is pre-trained, we keep the learning rate low and only perform a few epochs. This prevents it from overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2370049,
     "status": "ok",
     "timestamp": 1575308348148,
     "user": {
      "displayName": "Nick Newman",
      "photoUrl": "",
      "userId": "12535314105832740514"
     },
     "user_tz": 300
    },
    "id": "C1Bb31svWIQe",
    "outputId": "20499e55-5a1b-4344-88d0-ce2791f5a9b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/003 | Batch 001/406 | Average Loss in last 1 iteration(s): 0.7502\n",
      "Epoch: 001/003 | Batch 026/406 | Average Loss in last 25 iteration(s): 0.7033\n",
      "Epoch: 001/003 | Batch 051/406 | Average Loss in last 25 iteration(s): 0.6507\n",
      "Epoch: 001/003 | Batch 076/406 | Average Loss in last 25 iteration(s): 0.5867\n",
      "Epoch: 001/003 | Batch 101/406 | Average Loss in last 25 iteration(s): 0.5037\n",
      "Epoch: 001/003 | Batch 126/406 | Average Loss in last 25 iteration(s): 0.4174\n",
      "Epoch: 001/003 | Batch 151/406 | Average Loss in last 25 iteration(s): 0.3131\n",
      "Epoch: 001/003 | Batch 176/406 | Average Loss in last 25 iteration(s): 0.2610\n",
      "Epoch: 001/003 | Batch 201/406 | Average Loss in last 25 iteration(s): 0.2251\n",
      "Epoch: 001/003 | Batch 226/406 | Average Loss in last 25 iteration(s): 0.2310\n",
      "Epoch: 001/003 | Batch 251/406 | Average Loss in last 25 iteration(s): 0.2245\n",
      "Epoch: 001/003 | Batch 276/406 | Average Loss in last 25 iteration(s): 0.2011\n",
      "Epoch: 001/003 | Batch 301/406 | Average Loss in last 25 iteration(s): 0.1943\n",
      "Epoch: 001/003 | Batch 326/406 | Average Loss in last 25 iteration(s): 0.1766\n",
      "Epoch: 001/003 | Batch 351/406 | Average Loss in last 25 iteration(s): 0.1743\n",
      "Epoch: 001/003 | Batch 376/406 | Average Loss in last 25 iteration(s): 0.1859\n",
      "Epoch: 001/003 | Batch 401/406 | Average Loss in last 25 iteration(s): 0.1289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 406/406 [01:41<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Accuracy: 96.44%\n",
      "Epoch: 002/003 | Batch 001/406 | Average Loss in last 1 iteration(s): 0.0357\n",
      "Epoch: 002/003 | Batch 026/406 | Average Loss in last 25 iteration(s): 0.1523\n",
      "Epoch: 002/003 | Batch 051/406 | Average Loss in last 25 iteration(s): 0.1244\n",
      "Epoch: 002/003 | Batch 076/406 | Average Loss in last 25 iteration(s): 0.1014\n",
      "Epoch: 002/003 | Batch 101/406 | Average Loss in last 25 iteration(s): 0.1265\n",
      "Epoch: 002/003 | Batch 126/406 | Average Loss in last 25 iteration(s): 0.1290\n",
      "Epoch: 002/003 | Batch 151/406 | Average Loss in last 25 iteration(s): 0.0796\n",
      "Epoch: 002/003 | Batch 176/406 | Average Loss in last 25 iteration(s): 0.1167\n",
      "Epoch: 002/003 | Batch 201/406 | Average Loss in last 25 iteration(s): 0.0826\n",
      "Epoch: 002/003 | Batch 226/406 | Average Loss in last 25 iteration(s): 0.1022\n",
      "Epoch: 002/003 | Batch 251/406 | Average Loss in last 25 iteration(s): 0.1468\n",
      "Epoch: 002/003 | Batch 276/406 | Average Loss in last 25 iteration(s): 0.0828\n",
      "Epoch: 002/003 | Batch 301/406 | Average Loss in last 25 iteration(s): 0.0979\n",
      "Epoch: 002/003 | Batch 326/406 | Average Loss in last 25 iteration(s): 0.0998\n",
      "Epoch: 002/003 | Batch 351/406 | Average Loss in last 25 iteration(s): 0.1130\n",
      "Epoch: 002/003 | Batch 376/406 | Average Loss in last 25 iteration(s): 0.1274\n",
      "Epoch: 002/003 | Batch 401/406 | Average Loss in last 25 iteration(s): 0.0793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 406/406 [01:41<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Accuracy: 98.03%\n",
      "Epoch: 003/003 | Batch 001/406 | Average Loss in last 1 iteration(s): 0.0147\n",
      "Epoch: 003/003 | Batch 026/406 | Average Loss in last 25 iteration(s): 0.0896\n",
      "Epoch: 003/003 | Batch 051/406 | Average Loss in last 25 iteration(s): 0.0574\n",
      "Epoch: 003/003 | Batch 076/406 | Average Loss in last 25 iteration(s): 0.0628\n",
      "Epoch: 003/003 | Batch 101/406 | Average Loss in last 25 iteration(s): 0.0736\n",
      "Epoch: 003/003 | Batch 126/406 | Average Loss in last 25 iteration(s): 0.0842\n",
      "Epoch: 003/003 | Batch 151/406 | Average Loss in last 25 iteration(s): 0.0581\n",
      "Epoch: 003/003 | Batch 176/406 | Average Loss in last 25 iteration(s): 0.0861\n",
      "Epoch: 003/003 | Batch 201/406 | Average Loss in last 25 iteration(s): 0.0550\n",
      "Epoch: 003/003 | Batch 226/406 | Average Loss in last 25 iteration(s): 0.0692\n",
      "Epoch: 003/003 | Batch 251/406 | Average Loss in last 25 iteration(s): 0.1308\n",
      "Epoch: 003/003 | Batch 276/406 | Average Loss in last 25 iteration(s): 0.0439\n",
      "Epoch: 003/003 | Batch 301/406 | Average Loss in last 25 iteration(s): 0.0527\n",
      "Epoch: 003/003 | Batch 326/406 | Average Loss in last 25 iteration(s): 0.0760\n",
      "Epoch: 003/003 | Batch 351/406 | Average Loss in last 25 iteration(s): 0.0801\n",
      "Epoch: 003/003 | Batch 376/406 | Average Loss in last 25 iteration(s): 0.0846\n",
      "Epoch: 003/003 | Batch 401/406 | Average Loss in last 25 iteration(s): 0.0690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 406/406 [01:41<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Accuracy: 98.79%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache() #memory\n",
    "gc.collect() #memory\n",
    "NUM_EPOCHS = 3\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "losses = []\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-6)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    iteration = 0\n",
    "    for i, batch in enumerate(trainloader):\n",
    "        iteration += 1\n",
    "        token_ids, masks, labels = tuple(t.to(device) for t in batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss, yhat = model(input_ids=token_ids, attention_mask=masks, labels=labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += float(loss.item())\n",
    "        del token_ids, masks, labels #memory\n",
    "    \n",
    "        if not i%25:\n",
    "            print(f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "                  f'Batch {i+1:03d}/{len(trainloader):03d} | '\n",
    "                  f'Average Loss in last {iteration} iteration(s): {(running_loss/iteration):.4f}')\n",
    "            running_loss = 0.0\n",
    "            iteration = 0\n",
    "        torch.cuda.empty_cache() #memory\n",
    "        gc.collect() #memory\n",
    "        losses.append(float(loss.item()))\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'\\nTraining Accuracy: '\n",
    "              f'{compute_accuracy(model, trainloader, device):.2f}%')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ixSdyDXL1ww"
   },
   "source": [
    "#### Finally, we score the final model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2380145,
     "status": "ok",
     "timestamp": 1575308359643,
     "user": {
      "displayName": "Nick Newman",
      "photoUrl": "",
      "userId": "12535314105832740514"
     },
     "user_tz": 300
    },
    "id": "BAu6Y4pSL1Vf",
    "outputId": "3aeb9e1e-4b87-4dc3-d487-3f4242782897"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 46/46 [00:11<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Test Accuracy:96.36%\n"
     ]
    }
   ],
   "source": [
    "with torch.set_grad_enabled(False):\n",
    "  print(f'\\n\\nTest Accuracy:'\n",
    "  f'{compute_accuracy(model, testloader, device):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gxF5RZzgNmJF"
   },
   "source": [
    "#### We then do some error analysis by gathering the articles that were incorrectly predicted and analyzing the text of the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2389437,
     "status": "ok",
     "timestamp": 1575308371590,
     "user": {
      "displayName": "Nick Newman",
      "photoUrl": "",
      "userId": "12535314105832740514"
     },
     "user_tz": 300
    },
    "id": "u9jq78QlMpbL",
    "outputId": "2a16afed-c080-41ee-daa8-783f0cf686ed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:11<00:00,  4.04it/s]\n"
     ]
    }
   ],
   "source": [
    "test_predictions = torch.zeros((len(y_test),1))\n",
    "test_predictions_percent = torch.zeros((len(y_test),1))\n",
    "with torch.no_grad():\n",
    "  for i, batch in enumerate(tqdm(testloader)):\n",
    "    token_ids, masks, labels = tuple(t.to(device) for t in batch)\n",
    "    _, yhat = model(input_ids=token_ids, attention_mask=masks, labels=labels)\n",
    "    prediction = (torch.sigmoid(yhat[:,1]) > 0.5).long().view(-1,1)\n",
    "    test_predictions[i*BATCH_SIZE:(i+1)*BATCH_SIZE] = prediction\n",
    "    test_predictions_percent[i*BATCH_SIZE:(i+1)*BATCH_SIZE] = torch.sigmoid(yhat[:,1]).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qG-KPbFqMpih"
   },
   "outputs": [],
   "source": [
    "X_train_words, X_test_words, y_train_words, y_test_words = train_test_split(news_data['text'], target_variable, \n",
    "                                                    test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Phlxie3NMpnv"
   },
   "outputs": [],
   "source": [
    "final_results = X_test_words.to_frame().reset_index(drop=True)\n",
    "final_results['predicted'] = np.array(test_predictions.reshape(-1), dtype=int).tolist()\n",
    "final_results['percent'] = np.array(test_predictions_percent.reshape(-1), dtype=float).tolist()\n",
    "final_results['actual'] = y_test_words\n",
    "wrong_results = final_results.loc[final_results['predicted']!=final_results['actual']].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2388483,
     "status": "ok",
     "timestamp": 1575308371592,
     "user": {
      "displayName": "Nick Newman",
      "photoUrl": "",
      "userId": "12535314105832740514"
     },
     "user_tz": 300
    },
    "id": "28h0Xz3VMpq4",
    "outputId": "337f7ce9-c181-40c5-d2b3-025dde4fd2b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of incorrectly classified articles: 23\n"
     ]
    }
   ],
   "source": [
    "print('Number of incorrectly classified articles:', len(wrong_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yejVOpxzPSLc"
   },
   "source": [
    "#### This displays the incorrectly predicted instances, along with the percent confidence the algorithm had in each instance. The threshold for classification is 50%. Instances closer to 100% are more confident it's real news and instances closer to 0% are more confident it's fake news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2384347,
     "status": "ok",
     "timestamp": 1575308371766,
     "user": {
      "displayName": "Nick Newman",
      "photoUrl": "",
      "userId": "12535314105832740514"
     },
     "user_tz": 300
    },
    "id": "D7Wzg5EINQZv",
    "outputId": "84fdee6a-74d0-4395-a489-02ee19e60099"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row0_col0 {\n",
       "            width:  1000px;\n",
       "            white-space:  pre-wrap;\n",
       "        }    #T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row1_col0 {\n",
       "            width:  1000px;\n",
       "            white-space:  pre-wrap;\n",
       "        }    #T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row2_col0 {\n",
       "            width:  1000px;\n",
       "            white-space:  pre-wrap;\n",
       "        }    #T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row3_col0 {\n",
       "            width:  1000px;\n",
       "            white-space:  pre-wrap;\n",
       "        }    #T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row4_col0 {\n",
       "            width:  1000px;\n",
       "            white-space:  pre-wrap;\n",
       "        }    #T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row5_col0 {\n",
       "            width:  1000px;\n",
       "            white-space:  pre-wrap;\n",
       "        }    #T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row6_col0 {\n",
       "            width:  1000px;\n",
       "            white-space:  pre-wrap;\n",
       "        }    #T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row7_col0 {\n",
       "            width:  1000px;\n",
       "            white-space:  pre-wrap;\n",
       "        }    #T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row8_col0 {\n",
       "            width:  1000px;\n",
       "            white-space:  pre-wrap;\n",
       "        }    #T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row9_col0 {\n",
       "            width:  1000px;\n",
       "            white-space:  pre-wrap;\n",
       "        }    #T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row10_col0 {\n",
       "            width:  1000px;\n",
       "            white-space:  pre-wrap;\n",
       "        }    #T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row11_col0 {\n",
       "            width:  1000px;\n",
       "            white-space:  pre-wrap;\n",
       "        }    #T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row12_col0 {\n",
       "            width:  1000px;\n",
       "            white-space:  pre-wrap;\n",
       "        }    #T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row13_col0 {\n",
       "            width:  1000px;\n",
       "            white-space:  pre-wrap;\n",
       "        }    #T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row14_col0 {\n",
       "            width:  1000px;\n",
       "            white-space:  pre-wrap;\n",
       "        }    #T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row15_col0 {\n",
       "            width:  1000px;\n",
       "            white-space:  pre-wrap;\n",
       "        }    #T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row16_col0 {\n",
       "            width:  1000px;\n",
       "            white-space:  pre-wrap;\n",
       "        }    #T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row17_col0 {\n",
       "            width:  1000px;\n",
       "            white-space:  pre-wrap;\n",
       "        }    #T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row18_col0 {\n",
       "            width:  1000px;\n",
       "            white-space:  pre-wrap;\n",
       "        }    #T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row19_col0 {\n",
       "            width:  1000px;\n",
       "            white-space:  pre-wrap;\n",
       "        }    #T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row20_col0 {\n",
       "            width:  1000px;\n",
       "            white-space:  pre-wrap;\n",
       "        }    #T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row21_col0 {\n",
       "            width:  1000px;\n",
       "            white-space:  pre-wrap;\n",
       "        }    #T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row22_col0 {\n",
       "            width:  1000px;\n",
       "            white-space:  pre-wrap;\n",
       "        }</style><table id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >text_short</th>        <th class=\"col_heading level0 col1\" >percent</th>        <th class=\"col_heading level0 col2\" >predicted</th>        <th class=\"col_heading level0 col3\" >actual</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >78</th>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row0_col0\" class=\"data row0 col0\" >Imagine if, during the Jim Crow era, a newspaper offered advertisers the option of placing ads only in copies that went to white readers. \n",
       "That’s basically what Facebook is doing nowadays. \n",
       "The ubiquitous social network not only allows advertisers to target users by their interests or background, it also gives advertisers the ability to exclude specific groups it calls “Ethnic Affinities.” Ads that exclude people based on race, gender and other sensitive factors are prohibited by federal law in </td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row0_col1\" class=\"data row0 col1\" >0.886369</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row0_col2\" class=\"data row0 col2\" >1</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row0_col3\" class=\"data row0 col3\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >80</th>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row1_col0\" class=\"data row1 col0\" >USA Today \n",
       "WASHINGTON — The Army acknowledged Friday that Maj. Gen. John Rossi committed suicide on July 31, making him the highest-ranking soldier ever to have taken his own life. \n",
       "Rossi, who was 55, was just two days from pinning on his third star and taking command of Army Space and Missile Command when he killed himself at his home at Redstone Arsenal in Alabama. ‘ \n",
       "Investigators could find no event, infidelity, misconduct or drug or alcohol abuse, that triggered Rossi’s suicide, said a U.S.</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row1_col1\" class=\"data row1 col1\" >0.888966</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row1_col2\" class=\"data row1 col2\" >1</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row1_col3\" class=\"data row1 col3\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >117</th>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row2_col0\" class=\"data row2 col0\" >Unprecedented Surge In Election Fraud Incidents From Around The Country   Zero Hedge \n",
       "Mounting evidence would suggest it's getting more and more difficult for the left to claim that there are \"no signs\" of fraud in the 2016 election cycle...though we're sure they will continue to try. Just this morning the Miami Herald noted that two arrests were made in Miami-Dade county on election fraud charges including efforts by one woman to illegally register voters (some of whom were dead...a recurring t</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row2_col1\" class=\"data row2 col1\" >0.526852</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row2_col2\" class=\"data row2 col2\" >1</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row2_col3\" class=\"data row2 col3\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002level0_row3\" class=\"row_heading level0 row3\" >125</th>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row3_col0\" class=\"data row3 col0\" >Maggie Hassan, left and Kelly Ayotte Hassan declares victory in U.S. Senate race with Ayotte By PAUL FEELYNew Hampshire Union Leader Update, 11:00 a.m. Gov. Maggie Hassan declared she’s won New Hampshire's U.S. Senate race, unseating Republican Sen. Kelly Ayotte.During a hastily-called press conference outside the State House, Hassan said she’s ahead now by enough votes to survive returns from the few outstanding towns that are left.“I am proud to stand here as the next United States senator fro</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row3_col1\" class=\"data row3 col1\" >0.799829</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row3_col2\" class=\"data row3 col2\" >1</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row3_col3\" class=\"data row3 col3\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002level0_row4\" class=\"row_heading level0 row4\" >162</th>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row4_col0\" class=\"data row4 col0\" >  From the day we are born into this world, we are being taught what our parents have been taught, and what their parents have taught them, without asking many questions such as who we are, why we are here, and why things are the way they are. Existential questions are simply perceived as irrelevant in a left-brained society; in which money and career performance seem to be the primary focus. For those who seek a reason, countless financed religious institutions claim to provide the ultimate ans</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row4_col1\" class=\"data row4 col1\" >0.566883</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row4_col2\" class=\"data row4 col2\" >1</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row4_col3\" class=\"data row4 col3\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002level0_row5\" class=\"row_heading level0 row5\" >175</th>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row5_col0\" class=\"data row5 col0\" >(30 fans) - Advertisement - \n",
       "This article originally appeared at TomDispatch.com . To receive TomDispatch in your inbox three times a week, click here . \n",
       "Donald Trump has long campaigned on the promise of running the country the way he's run his businesses. On that basis, we essentially already know what it would mean if he entered the Oval Office and applied his personal business acumen to this nation (and the rest of the world). There's a surprisingly full record to cite. Who can forget, for i</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row5_col1\" class=\"data row5 col1\" >0.628271</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row5_col2\" class=\"data row5 col2\" >1</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row5_col3\" class=\"data row5 col3\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002level0_row6\" class=\"row_heading level0 row6\" >200</th>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row6_col0\" class=\"data row6 col0\" >\n",
       "Fox News reported : \n",
       "Five police officers and yellow caution tape surround Donald Trump’s Hollywood Walk of Fame star — or what’s left of it. \n",
       "The Los Angeles police say they are investigating the smashing of Trump’s star following footage that showed the sidewalk tribute was destroyed with a pickax. \n",
       "Det. Meghan Aguilar says investigators were called to the scene before dawn Wednesday. \n",
       "By mid-morning, an LAPD spokesperson at the scene told FOX411 the Chamber of Commerce was sending out a crew</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row6_col1\" class=\"data row6 col1\" >0.661488</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row6_col2\" class=\"data row6 col2\" >1</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row6_col3\" class=\"data row6 col3\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002level0_row7\" class=\"row_heading level0 row7\" >210</th>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row7_col0\" class=\"data row7 col0\" > especially the conservatives. It’s Independents like Sanders who will fight for our rights…people who are not bought by the power elite.\"</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row7_col1\" class=\"data row7 col1\" >0.855685</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row7_col2\" class=\"data row7 col2\" >1</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row7_col3\" class=\"data row7 col3\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002level0_row8\" class=\"row_heading level0 row8\" >242</th>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row8_col0\" class=\"data row8 col0\" >On The Streets Of Baltimore, Trying To Understand The Anger\n",
       "\n",
       "In the early morning, as the cold set in, Anaya Maze stood next to the charred remains of a CVS store.\n",
       "\n",
       "Holding a sign, she was the only protester left in front of a line of police officers dressed in riot gear. She is petite. Still, she faced the police officers, looking at them intently.\n",
       "\n",
       "A few steps away were the charred skeletons of two police vehicles, the victims of an unbridled anger that burned its way through the west side of </td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row8_col1\" class=\"data row8 col1\" >0.418241</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row8_col2\" class=\"data row8 col2\" >0</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row8_col3\" class=\"data row8 col3\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002level0_row9\" class=\"row_heading level0 row9\" >287</th>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row9_col0\" class=\"data row9 col0\" >  \n",
       "With Hillary Clinton making history this election season by becoming the first women nominated by a major party, the sexism has been on full display. The misogynists on the Right have questioned her health, her stamina, and everything in between. That is, of course, code for “the little woman doesn’t belong in the Oval Office.” However, one Texas Republican has taken the misogyny to a whole other level. \n",
       "Meet Sid Miller, the Agriculture Commissioner for the state of Texas. He is also a former</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row9_col1\" class=\"data row9 col1\" >0.787238</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row9_col2\" class=\"data row9 col2\" >1</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row9_col3\" class=\"data row9 col3\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002level0_row10\" class=\"row_heading level0 row10\" >304</th>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row10_col0\" class=\"data row10 col0\" >  \n",
       "Republicans just can’t quit fantasizing about doing bodily harm to Hillary Clinton. Over the weekend, Sen. Richard Burr noticed a picture of Clinton hanging in a gun shop and decided the owners should put a “bullseye” on it. \n",
       "Speaking to GOP volunteers on Saturday , Burr joked that he had recently visited a gun shop and “nothing made me feel better” than seeing a magazine about rifles“with a picture of Hillary Clinton on the front of it.” \n",
       "“I was a little bit shocked at that — it didn’t have </td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row10_col1\" class=\"data row10 col1\" >0.679939</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row10_col2\" class=\"data row10 col2\" >1</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row10_col3\" class=\"data row10 col3\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002level0_row11\" class=\"row_heading level0 row11\" >313</th>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row11_col0\" class=\"data row11 col0\" >Robert Spencer \n",
       "Breitbart reported Saturday that among the many revealing and damning emails that WikiLeaks has revealed from Clinton campaign chairman John Podesta, one has “White House chief of staff Denis McDonough responding favorably to an email forwarded to him by Podesta from a leftwing ‘Catholic’ organization that said it was arranging meetings with Catholic prelates to urge them to press U.S. senators to vote for the Iran Treaty.” \n",
       "This plan was apparently hatched by Fred Rotondaro, cha</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row11_col1\" class=\"data row11 col1\" >0.637295</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row11_col2\" class=\"data row11 col2\" >1</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row11_col3\" class=\"data row11 col3\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002level0_row12\" class=\"row_heading level0 row12\" >329</th>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row12_col0\" class=\"data row12 col0\" >It’s no secret that Obama and his puppets are doing everything they possibly can to rig the election against Trump. Positive Trump polls are coming all the time, so it’s all hands on deck from the Democrats to do whatever it takes to hand the election to Hillary.\n",
       "Obama’s latest move involves registering immigrant voters who will most likely be voting for Hillary in November and he’s spent TONS of your taxpayer money doing it.\n",
       "From Judicial Watch :\n",
       "Months after the Obama administration spent $19 </td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row12_col1\" class=\"data row12 col1\" >0.873888</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row12_col2\" class=\"data row12 col2\" >1</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row12_col3\" class=\"data row12 col3\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002level0_row13\" class=\"row_heading level0 row13\" >330</th>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row13_col0\" class=\"data row13 col0\" >  \n",
       "  block the US from being stationed on the islands off Hokkaido in the strategic Sea of Okhotsk, if this helps persuade Russia to give them back. \n",
       "The islands are inhabited, and in Russia are called the Southern Kurils – but for Japan they are Etorofu, Kunashiri, Shikotan, and the Habomai islet group. These territories, which became Russian after Japan’s defeat in World War II under the San Francisco Peace Treaty of 1951, saw a rift between the two countries preventing them from signing the p</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row13_col1\" class=\"data row13 col1\" >0.876478</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row13_col2\" class=\"data row13 col2\" >1</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row13_col3\" class=\"data row13 col3\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002level0_row14\" class=\"row_heading level0 row14\" >370</th>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row14_col0\" class=\"data row14 col0\" >Research could ‘potentially serve as a curative approach for patients with HIV’, scientist says \n",
       "\n",
       "Scientists have managed to remove DNA of the HIV virus from living tissue for the first time in a breakthrough that could lead to an outright cure.\n",
       "\n",
       "At the moment, treating the disease involves the use of drugs that suppress levels of the virus so the body’s immune system can cope.\n",
       "\n",
       "Now researchers in the US have revealed they used gene-editing technology to remove DNA of the commonest HIV-1 strain </td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row14_col1\" class=\"data row14 col1\" >0.642608</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row14_col2\" class=\"data row14 col2\" >1</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row14_col3\" class=\"data row14 col3\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002level0_row15\" class=\"row_heading level0 row15\" >383</th>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row15_col0\" class=\"data row15 col0\" >  America is going nuts. Suggestions stupidly abound about how Russian hackers are even going so far as to read people's emails. Of course there is absolutely zero evidence of this. Russians are being blamed for everything from flooded toilets, biting dogs, the tornadoes in Kansas, the kid's messy diapers, the car won't start and grandpa lost his dentures. Media encouraged hysteria This hysteria is being encouraged by the media, and especially pretentious, arrogant dishonest politicians and ever</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row15_col1\" class=\"data row15 col1\" >0.940817</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row15_col2\" class=\"data row15 col2\" >1</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row15_col3\" class=\"data row15 col3\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002level0_row16\" class=\"row_heading level0 row16\" >395</th>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row16_col0\" class=\"data row16 col0\" >\n",
       "OK, theoretically, everything will go according to plan, and Donald Trump will be the next president.\n",
       "But technically, the (s)election hasn’t really taken place yet.\n",
       "Presidential electors of the mystified electoral college must still actually vote for the president, and there isn’t anything to keep them from ‘voting their conscience’ and choosing someone other than Donald Trump.\n",
       "Moreover, it appears that there is an active effort to flip the electoral college to deny Trump the presidency, and t</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row16_col1\" class=\"data row16 col1\" >0.928357</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row16_col2\" class=\"data row16 col2\" >1</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row16_col3\" class=\"data row16 col3\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002level0_row17\" class=\"row_heading level0 row17\" >410</th>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row17_col0\" class=\"data row17 col0\" >\n",
       "\n",
       "CNN (also known as the Clinton News Network) has gone out of its way to try to rig the election for Democrat nominee Hillary Clinton this year. It hasn’t even tried to hide its bias in an attempt to destroy Republican nominee Donald Trump’s chances at becoming president.\n",
       "Over the weekend, CNN tried desperately to find anything it could to distract the American people from the FBI’s announcement that it was reopening the investigation into Clinton’s use of an email server while she was secretar</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row17_col1\" class=\"data row17 col1\" >0.869428</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row17_col2\" class=\"data row17 col2\" >1</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row17_col3\" class=\"data row17 col3\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002level0_row18\" class=\"row_heading level0 row18\" >495</th>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row18_col0\" class=\"data row18 col0\" >Let's pretend for a moment that the biggest headlines out of Sunday night's presidential debate had nothing to do with sexual assault allegations, or non-handshakes, or threats to jail political opponents—but instead were about policy. In that bizarre alternative universe, what could we actually learn?\n",
       "\n",
       "That the two exhausted political parties have nothing much left to offer except critiques about how lousy the other one is.\n",
       "\n",
       "Hosted by Matt Welch; camera and editing by Jim Epstein.\n",
       "\n",
       "Like us on F</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row18_col1\" class=\"data row18 col1\" >0.322254</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row18_col2\" class=\"data row18 col2\" >0</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row18_col3\" class=\"data row18 col3\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002level0_row19\" class=\"row_heading level0 row19\" >512</th>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row19_col0\" class=\"data row19 col0\" >The Millennial Vote is being treated like a Magical Unicorn in the 2016 election. It is seen as something valuable and mysterious. As Dan Schwabel, at Quartz, in a piece modestly entitled The complete guide to winning the millennial vote this election recently noted:\n",
       "\n",
       "As we head into November’s US elections, all candidates are vying for the millennial vote—and for good reason. Millennials are ... a critical bloc for any campaign. 69.2 million are now eligible to vote, which is more than double c</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row19_col1\" class=\"data row19 col1\" >0.421001</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row19_col2\" class=\"data row19 col2\" >0</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row19_col3\" class=\"data row19 col3\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002level0_row20\" class=\"row_heading level0 row20\" >534</th>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row20_col0\" class=\"data row20 col0\" >The latest batch of emails released by WikiLeaks provides a rare glimpse into how Democratic nominee Hillary Clinton’s campaign handles money from U.S. lobbyists who are registered agents for foreign interests.\n",
       "\n",
       "In an email chain with the subject, “Re: Foreign registered agents,” various figures in her presidential campaign discuss the best way to handle donations from U.S. lobbyists who are registered agents for foreign parties.\n",
       "\n",
       "The chain features Dennis Cheng, national finance director for th</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row20_col1\" class=\"data row20 col1\" >0.158641</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row20_col2\" class=\"data row20 col2\" >0</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row20_col3\" class=\"data row20 col3\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002level0_row21\" class=\"row_heading level0 row21\" >541</th>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row21_col0\" class=\"data row21 col0\" >Former United States Attorney for the District of Columbia Joe diGenova suggested Friday James Comey’s reopening of the investigation into Hillary Clinton’s emails was the product of a “revolt” inside the FBI. \n",
       "While speaking with WMAL’s Larry O’Connor , diGenova was asked what Comey meant by saying newly discovered Clinton emails from Anthony Weiner and Huma Abedin’s devices “appear to be pertinent” to the FBI’s prior investigation. \n",
       "“It tells me that the original investigation was not thorough</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row21_col1\" class=\"data row21 col1\" >0.645675</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row21_col2\" class=\"data row21 col2\" >1</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row21_col3\" class=\"data row21 col3\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002level0_row22\" class=\"row_heading level0 row22\" >571</th>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row22_col0\" class=\"data row22 col0\" >Happy Birthday, Hillary. You were destined to great things. And you knew it. *** I am astounded . I graduated in political science from the University of Naples, the university of Saint Thomas Aquinas, Giambattista Vico, and Benedetto Croce, in something like the 750 th graduating class; I have tried to keep up with the field as much as I could, even though I have preferred to concentrate on economics and political economy. I have always gotten along with the assumption that politics is \"the art</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row22_col1\" class=\"data row22 col1\" >0.779982</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row22_col2\" class=\"data row22 col2\" >1</td>\n",
       "                        <td id=\"T_b23b4a60_152a_11ea_a7c2_0242ac1c0002row22_col3\" class=\"data row22 col3\" >0</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fd379872c50>"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_results.loc[:,'text_short'] = wrong_results.loc[:,'text'].apply(lambda x: x[:500])\n",
    "wrong_results.loc[:,('text_short', 'percent','predicted','actual')].style.set_properties(subset=['text_short'], **{'width': '1000px', 'white-space':'pre-wrap'})"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "BERT_fakeNews.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
